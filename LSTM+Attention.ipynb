{"cells":[{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2867,"status":"ok","timestamp":1733298748789,"user":{"displayName":"김민서","userId":"06054877758271253489"},"user_tz":-540},"id":"BNV1dZZLqNZm","outputId":"92e366ef-6857-445c-95f6-fbb17f0c6bda"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30359,"status":"ok","timestamp":1733298780841,"user":{"displayName":"김민서","userId":"06054877758271253489"},"user_tz":-540},"id":"mh5JzwtVqQav","outputId":"69a748f8-b807-4997-e8d7-0f3b168470f9"},"outputs":[{"name":"stdout","output_type":"stream","text":["replace sample_submission.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n","replace test/meta/TEST_산지공판장_00.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "]}],"source":["!unzip -qq \"/content/drive/MyDrive/DS/open.zip\""]},{"cell_type":"code","execution_count":41,"metadata":{"executionInfo":{"elapsed":430,"status":"ok","timestamp":1733298787608,"user":{"displayName":"김민서","userId":"06054877758271253489"},"user_tz":-540},"id":"1SvyhYaapgwo"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from tqdm.notebook import tqdm\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","from types import SimpleNamespace\n","from sklearn.preprocessing import MinMaxScaler\n","import os"]},{"cell_type":"code","execution_count":63,"metadata":{"executionInfo":{"elapsed":336,"status":"ok","timestamp":1733299429235,"user":{"displayName":"김민서","userId":"06054877758271253489"},"user_tz":-540},"id":"hxMK5sM7poSk"},"outputs":[],"source":["config = {\n","    \"learning_rate\": 0.0001,\n","    \"epoch\": 100,\n","    \"batch_size\": 64,\n","    \"hidden_size\": 128,\n","    \"num_layers\": 4,\n","    \"output_size\": 3,\n","    \"num_heads\": 4,\n","    \"dropout\": 0.2,\n","    \"step_size\": 10,\n","    \"gamma\": 0.8\n","}\n","\n","CFG = SimpleNamespace(**config)\n","\n","item_list = ['건고추', '사과', '감자', '배', '깐마늘(국산)', '무', '상추', '배추', '양파', '대파']"]},{"cell_type":"code","execution_count":54,"metadata":{"executionInfo":{"elapsed":429,"status":"ok","timestamp":1733299223299,"user":{"displayName":"김민서","userId":"06054877758271253489"},"user_tz":-540},"id":"fI7EZxgEiops"},"outputs":[],"source":["import re\n","from datetime import datetime\n","\n","def parse_custom_date(date_str):\n","    # 접두사 'T-' 제거\n","    if date_str.startswith('T-'):\n","        date_str = date_str.lstrip('T-')\n","\n","    # 정규식으로 날짜를 파싱\n","    match = re.match(r\"(\\d{4})(\\d{2})(상순|중순|하순)\", date_str)\n","    if match:\n","        year, month, part = match.groups()\n","        # 상순, 중순, 하순을 각각 1일, 11일, 21일로 매핑\n","        day = {\"상순\": \"01\", \"중순\": \"11\", \"하순\": \"21\"}[part]\n","        # 날짜 생성\n","        return datetime.strptime(f\"{year}-{month}-{day}\", \"%Y-%m-%d\")\n","    else:\n","        return None\n"]},{"cell_type":"code","execution_count":55,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1733299223656,"user":{"displayName":"김민서","userId":"06054877758271253489"},"user_tz":-540},"id":"H32S4lD_9Y7o"},"outputs":[],"source":["def process_data(raw_file, 산지공판장_file, 전국도매_file, 품목명, scaler=None):\n","    raw_data = pd.read_csv(raw_file)\n","    산지공판장 = pd.read_csv(산지공판장_file)\n","    전국도매 = pd.read_csv(전국도매_file)\n","\n","    # 타겟 및 메타데이터 필터 조건 정의\n","    conditions = {\n","    '감자': {\n","        'target': lambda df: (df['품종명'] == '감자 수미') \u0026 (df['거래단위'] == '20키로상자') \u0026 (df['등급'] == '상'),\n","        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['감자'], '품종명': ['수미'], '등급명': ['상']},\n","        '도매': {'시장명': ['*전국도매시장'], '품목명': ['감자'], '품종명': ['수미']}\n","    },\n","    '건고추': {\n","        'target': lambda df: (df['품종명'] == '화건') \u0026 (df['거래단위'] == '30 kg') \u0026 (df['등급'] == '상품'),\n","        '공판장': None,\n","        '도매': None\n","    },\n","    '깐마늘(국산)': {\n","        'target': lambda df: (df['거래단위'] == '20 kg') \u0026 (df['등급'] == '상품'),\n","        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['마늘'], '품종명': ['깐마늘'], '등급명': ['상']},\n","        '도매': {'시장명': ['*전국도매시장'], '품목명': ['마늘'], '품종명': ['깐마늘']}\n","    },\n","    '대파': {\n","        'target': lambda df: (df['품종명'] == '대파(일반)') \u0026 (df['거래단위'] == '1키로단') \u0026 (df['등급'] == '상'),\n","        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['대파'], '품종명': ['대파(일반)'], '등급명': ['상']},\n","        '도매': {'시장명': ['*전국도매시장'], '품목명': ['대파'], '품종명': ['대파(일반)']}\n","    },\n","    '무': {\n","        'target': lambda df: (df['거래단위'] == '20키로상자') \u0026 (df['등급'] == '상'),\n","        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['무'], '품종명': ['기타무'], '등급명': ['상']},\n","        '도매': {'시장명': ['*전국도매시장'], '품목명': ['무'], '품종명': ['무']}\n","    },\n","    '배추': {\n","        'target': lambda df: (df['거래단위'] == '10키로망대') \u0026 (df['등급'] == '상'),\n","        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['배추'], '품종명': ['쌈배추'], '등급명': ['상']},\n","        '도매': {'시장명': ['*전국도매시장'], '품목명': ['배추'], '품종명': ['배추']}\n","    },\n","    '사과': {\n","        'target': lambda df: (df['품종명'].isin(['홍로', '후지'])) \u0026 (df['거래단위'] == '10 개') \u0026 (df['등급'] == '상품'),\n","        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['사과'], '품종명': ['후지'], '등급명': ['상']},\n","        '도매': {'시장명': ['*전국도매시장'], '품목명': ['사과'], '품종명': ['후지']}\n","    },\n","    '상추': {\n","        'target': lambda df: (df['품종명'] == '청') \u0026 (df['거래단위'] == '100 g') \u0026 (df['등급'] == '상품'),\n","        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['상추'], '품종명': ['청상추'], '등급명': ['상']},\n","        '도매': {'시장명': ['*전국도매시장'], '품목명': ['상추'], '품종명': ['청상추']}\n","    },\n","    '양파': {\n","        'target': lambda df: (df['품종명'] == '양파') \u0026 (df['거래단위'] == '1키로') \u0026 (df['등급'] == '상'),\n","        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['양파'], '품종명': ['기타양파'], '등급명': ['상']},\n","        '도매': {'시장명': ['*전국도매시장'], '품목명': ['양파'], '품종명': ['양파(일반)']}\n","    },\n","    '배': {\n","        'target': lambda df: (df['품종명'] == '신고') \u0026 (df['거래단위'] == '10 개') \u0026 (df['등급'] == '상품'),\n","        '공판장': {'공판장명': ['*전국농협공판장'], '품목명': ['배'], '품종명': ['신고'], '등급명': ['상']},\n","        '도매': {'시장명': ['*전국도매시장'], '품목명': ['배'], '품종명': ['신고']}\n","    }\n","    }\n","\n","    # 타겟 데이터 필터링\n","    raw_품목 = raw_data[raw_data['품목명'] == 품목명]\n","    target_mask = conditions[품목명]['target'](raw_품목)\n","    filtered_data = raw_품목[target_mask]\n","\n","    # 다른 품종에 대한 파생변수 생성\n","    other_data = raw_품목[~target_mask]\n","    unique_combinations = other_data[['품종명', '거래단위', '등급']].drop_duplicates()\n","    for _, row in unique_combinations.iterrows():\n","        품종명, 거래단위, 등급 = row['품종명'], row['거래단위'], row['등급']\n","        mask = (other_data['품종명'] == 품종명) \u0026 (other_data['거래단위'] == 거래단위) \u0026 (other_data['등급'] == 등급)\n","        temp_df = other_data[mask]\n","        for col in ['평년 평균가격(원)', '평균가격(원)']:\n","            new_col_name = f'{품종명}_{거래단위}_{등급}_{col}'\n","            filtered_data = filtered_data.merge(temp_df[['시점', col]], on='시점', how='left', suffixes=('', f'_{new_col_name}'))\n","            filtered_data.rename(columns={f'{col}_{new_col_name}': new_col_name}, inplace=True)\n","\n","\n","    # 공판장 데이터 처리\n","    if conditions[품목명]['공판장']:\n","        filtered_공판장 = 산지공판장\n","        for key, value in conditions[품목명]['공판장'].items():\n","            filtered_공판장 = filtered_공판장[filtered_공판장[key].isin(value)]\n","\n","        filtered_공판장 = filtered_공판장.add_prefix('공판장_').rename(columns={'공판장_시점': '시점'})\n","        filtered_data = filtered_data.merge(filtered_공판장, on='시점', how='left')\n","\n","    # 도매 데이터 처리\n","    if conditions[품목명]['도매']:\n","        filtered_도매 = 전국도매\n","        for key, value in conditions[품목명]['도매'].items():\n","            filtered_도매 = filtered_도매[filtered_도매[key].isin(value)]\n","\n","        filtered_도매 = filtered_도매.add_prefix('도매_').rename(columns={'도매_시점': '시점'})\n","        filtered_data = filtered_data.merge(filtered_도매, on='시점', how='left')\n","\n","    ### 날짜변환\n","    filtered_data['시점'] = filtered_data['시점'].apply(parse_custom_date)\n","\n","    # 수치형 컬럼 처리\n","    numeric_columns = filtered_data.select_dtypes(include=[np.number]).columns\n","    filtered_data = filtered_data[['시점'] + list(numeric_columns)]\n","    filtered_data[numeric_columns] = filtered_data[numeric_columns].fillna(0)\n","\n","    ### 결측치 처리: 선형 보간법\n","    filtered_data[numeric_columns] = filtered_data[numeric_columns].interpolate(method='linear', limit_direction='both')\n","\n","    ### 이상치 처리: IQR 방법 ###\n","#    for col in numeric_columns:\n","#        Q1 = filtered_data[col].quantile(0.25)\n","#        Q3 = filtered_data[col].quantile(0.75)\n","#        IQR = Q3 - Q1\n","#        lower_bound = Q1 - 1.5 * IQR\n","#        upper_bound = Q3 + 1.5 * IQR\n","#        filtered_data = filtered_data[(filtered_data[col] \u003e= lower_bound) \u0026 (filtered_data[col] \u003c= upper_bound)]\n","    #######\n","\n","#####mentori님의 이상치 처리 부분 참######\n","    for col in numeric_columns:\n","        for i in range(1, len(filtered_data)):\n","            if filtered_data.loc[i, col] == 0:  # 0인 값을 이상치로 간주\n","                filtered_data.loc[i, col] = filtered_data.loc[i - 1, col]  # 이전 값으로 대체\n","\n","    # 평균가격 값이 0인 것이 일정 수준 이하라면 그 칼럼을 제거한다.\n","    drop_columns = []\n","    for col in filtered_data.columns:\n","        zero_cols = len(filtered_data[filtered_data[col]==0])\n","        if zero_cols/len(filtered_data) \u003e 0.5:\n","            drop_columns.append(col)\n","###################################################\n","\n","    # 정규화 적용\n","    if scaler is None:\n","        scaler = MinMaxScaler()\n","        filtered_data[numeric_columns] = scaler.fit_transform(filtered_data[numeric_columns])\n","    else:\n","        filtered_data[numeric_columns] = scaler.transform(filtered_data[numeric_columns])\n","\n","    return filtered_data, scaler\n"]},{"cell_type":"code","execution_count":56,"metadata":{"executionInfo":{"elapsed":343,"status":"ok","timestamp":1733299226166,"user":{"displayName":"김민서","userId":"06054877758271253489"},"user_tz":-540},"id":"Upyu8YjK9a8j"},"outputs":[],"source":["# Define custom dataset class\n","class AgriculturePriceDataset(Dataset):\n","    def __init__(self, dataframe, window_size=9, prediction_length=3, is_test=False):\n","        self.data = dataframe\n","        self.window_size = window_size\n","        self.prediction_length = prediction_length\n","        self.is_test = is_test\n","\n","        self.price_column = [col for col in self.data.columns if '평균가격(원)' in col and len(col.split('_')) == 1][0]\n","        self.numeric_columns = self.data.select_dtypes(include=[np.number]).columns.tolist()\n","\n","        self.sequences = []\n","        if not self.is_test:\n","            for i in range(len(self.data) - self.window_size - self.prediction_length + 1):\n","                x = self.data[self.numeric_columns].iloc[i:i+self.window_size].values\n","                y = self.data[self.price_column].iloc[i+self.window_size:i+self.window_size+self.prediction_length].values\n","                self.sequences.append((x, y))\n","        else:\n","            self.sequences = [self.data[self.numeric_columns].values]\n","\n","    def __len__(self):\n","        return len(self.sequences)\n","\n","    def __getitem__(self, idx):\n","        if not self.is_test:\n","            x, y = self.sequences[idx]\n","            return torch.FloatTensor(x), torch.FloatTensor(y)\n","        else:\n","            return torch.FloatTensor(self.sequences[idx])"]},{"cell_type":"code","execution_count":57,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1733299227530,"user":{"displayName":"김민서","userId":"06054877758271253489"},"user_tz":-540},"id":"VPPgQd4U9dNx"},"outputs":[],"source":["# Define Time2Vec layer\n","class Time2Vec(nn.Module):\n","    def __init__(self, input_dim):\n","        super(Time2Vec, self).__init__()\n","        self.linear = nn.Linear(input_dim, 1)\n","        self.periodic = nn.Linear(input_dim, input_dim-1)\n","\n","    def forward(self, x):\n","        linear_out = self.linear(x)\n","        periodic_out = torch.sin(self.periodic(x))\n","        return torch.cat([linear_out, periodic_out], dim=-1)\n","\n","# Define Transformer Encoder Block\n","class TransformerBlock(nn.Module):\n","    def __init__(self, input_dim, num_heads, dropout):\n","        super(TransformerBlock, self).__init__()\n","        self.attention = nn.MultiheadAttention(input_dim, num_heads, dropout=dropout)\n","        self.norm1 = nn.LayerNorm(input_dim)\n","        self.norm2 = nn.LayerNorm(input_dim)\n","        self.ff = nn.Sequential(\n","            nn.Linear(input_dim, 4 * input_dim),\n","            nn.GELU(),\n","            nn.Linear(4 * input_dim, input_dim),\n","            nn.Dropout(dropout)\n","        )\n","\n","    def forward(self, x):\n","        attended, _ = self.attention(x, x, x)\n","        x = self.norm1(attended + x)\n","        feedforward = self.ff(x)\n","        return self.norm2(feedforward + x)\n","\n","# Define main model architecture\n","class TimeSeriesTransformer(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, num_heads, output_size, dropout):\n","        super(TimeSeriesTransformer, self).__init__()\n","        self.time2vec = Time2Vec(input_size)\n","        self.embedding = nn.Linear(input_size, hidden_size)\n","        self.position_encoding = self.generate_position_encoding(hidden_size, 10)\n","        self.dropout = nn.Dropout(dropout)\n","\n","        self.transformer_blocks = nn.ModuleList([\n","            TransformerBlock(hidden_size, num_heads, dropout)\n","            for _ in range(num_layers)\n","        ])\n","\n","        self.output_layer = nn.Sequential(\n","            nn.Linear(hidden_size, hidden_size // 2),\n","            nn.ReLU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(hidden_size // 2, output_size)\n","        )\n","    def generate_position_encoding(self, hidden_size, max_len):\n","        pe = torch.zeros(max_len, hidden_size)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, hidden_size, 2).float() * (-np.log(10000.0) / hidden_size))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        return pe.unsqueeze(0)\n","\n","    def forward(self, x):\n","        b, s, f = x.shape\n","        x = self.time2vec(x)\n","        x = self.embedding(x)\n","        x = x + self.position_encoding[:, :s, :].to(x.device)\n","        x = self.dropout(x)\n","\n","        for transformer in self.transformer_blocks:\n","            x = transformer(x)\n","\n","        x = x.mean(dim=1)\n","        return self.output_layer(x)\n"]},{"cell_type":"code","execution_count":58,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1733299228937,"user":{"displayName":"김민서","userId":"06054877758271253489"},"user_tz":-540},"id":"Yssl_tSikIIS"},"outputs":[],"source":["# Training function with mixed precision training\n","def train_model(model, train_loader, criterion, optimizer, scheduler, scaler, device):\n","    model.train()\n","    total_loss = 0\n","    for batch_x, batch_y in train_loader:\n","        batch_x, batch_y = batch_x.to(device), batch_y.to(device)  # 데이터를 GPU로 이동\n","        optimizer.zero_grad()\n","\n","        with torch.cuda.amp.autocast():  # GPU에서 Mixed Precision Training\n","            outputs = model(batch_x)\n","            loss = criterion(outputs, batch_y)\n","\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        scheduler.step()\n","        total_loss += loss.item()\n","\n","    return total_loss / len(train_loader)\n"]},{"cell_type":"code","execution_count":59,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1733299230150,"user":{"displayName":"김민서","userId":"06054877758271253489"},"user_tz":-540},"id":"wUzQ5dJ1kLPK"},"outputs":[],"source":["# Evaluation function\n","def evaluate_model(model, test_loader, criterion, device):\n","    model.eval()\n","    total_loss = 0\n","    with torch.no_grad():\n","        for batch_x, batch_y in test_loader:\n","            batch_x, batch_y = batch_x.to(device), batch_y.to(device)  # 데이터를 GPU로 이동\n","            outputs = model(batch_x)\n","            loss = criterion(outputs, batch_y)\n","            total_loss += loss.item()\n","    return total_loss / len(test_loader)"]},{"cell_type":"code","execution_count":60,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1733299231517,"user":{"displayName":"김민서","userId":"06054877758271253489"},"user_tz":-540},"id":"gtlSeSxilGc8"},"outputs":[],"source":["class NMAELoss(nn.Module):\n","    def __init__(self):\n","        super(NMAELoss, self).__init__()\n","\n","    def forward(self, y_pred, y_true):\n","        mae = torch.abs(y_pred - y_true).mean()\n","        # y_true의 평균을 기준으로 정규화\n","        nmae = mae / torch.mean(torch.abs(y_true))\n","        return nmae"]},{"cell_type":"code","execution_count":61,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1733299232870,"user":{"displayName":"김민서","userId":"06054877758271253489"},"user_tz":-540},"id":"fMyjZfAglc8j"},"outputs":[],"source":["import torch\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000},"id":"xf0PQGtt9e31"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d5ad6fdced5b49129c4f0ce94251b9ff","version_major":2,"version_minor":0},"text/plain":["품목 처리 중:   0%|          | 0/10 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u003cipython-input-66-63ebe08d55c0\u003e:41: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = GradScaler()\n","\u003cipython-input-58-603e94c8aebc\u003e:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():  # GPU에서 Mixed Precision Training\n","/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/100, Train Loss: 1.1739\n","Epoch 2/100, Train Loss: 1.0024\n","Epoch 3/100, Train Loss: 0.5657\n","Epoch 4/100, Train Loss: 0.6564\n","Epoch 5/100, Train Loss: 0.6307\n","Epoch 6/100, Train Loss: 0.5614\n","Epoch 7/100, Train Loss: 0.5271\n","Epoch 8/100, Train Loss: 0.4690\n","Epoch 9/100, Train Loss: 0.4916\n","Epoch 10/100, Train Loss: 0.4690\n","Epoch 11/100, Train Loss: 0.5063\n","Epoch 12/100, Train Loss: 0.5271\n","Epoch 13/100, Train Loss: 0.4813\n","Epoch 14/100, Train Loss: 0.5100\n","Epoch 15/100, Train Loss: 0.4746\n","Epoch 16/100, Train Loss: 0.4864\n","Epoch 17/100, Train Loss: 0.4604\n","Epoch 18/100, Train Loss: 0.4771\n","Epoch 19/100, Train Loss: 0.4475\n","Epoch 20/100, Train Loss: 0.4592\n","Epoch 21/100, Train Loss: 0.4765\n","Epoch 22/100, Train Loss: 0.4289\n","Epoch 23/100, Train Loss: 0.4451\n","Epoch 24/100, Train Loss: 0.4200\n","Epoch 25/100, Train Loss: 0.4241\n","Epoch 26/100, Train Loss: 0.4820\n","Epoch 27/100, Train Loss: 0.4522\n","Epoch 28/100, Train Loss: 0.4468\n","Epoch 29/100, Train Loss: 0.4640\n","Epoch 30/100, Train Loss: 0.4725\n","Epoch 31/100, Train Loss: 0.4259\n","Epoch 32/100, Train Loss: 0.4113\n","Epoch 33/100, Train Loss: 0.4035\n","Epoch 34/100, Train Loss: 0.4346\n","Epoch 35/100, Train Loss: 0.4435\n","Epoch 36/100, Train Loss: 0.4295\n","Epoch 37/100, Train Loss: 0.4360\n","Epoch 38/100, Train Loss: 0.4625\n","Epoch 39/100, Train Loss: 0.3864\n","Epoch 40/100, Train Loss: 0.4523\n","Epoch 41/100, Train Loss: 0.4310\n","Epoch 42/100, Train Loss: 0.4258\n","Epoch 43/100, Train Loss: 0.4082\n","Epoch 44/100, Train Loss: 0.4127\n","Epoch 45/100, Train Loss: 0.4145\n","Epoch 46/100, Train Loss: 0.4046\n","Epoch 47/100, Train Loss: 0.4109\n","Epoch 48/100, Train Loss: 0.3928\n","Epoch 49/100, Train Loss: 0.4142\n","Epoch 50/100, Train Loss: 0.3854\n","Epoch 51/100, Train Loss: 0.4173\n","Epoch 52/100, Train Loss: 0.4123\n","Epoch 53/100, Train Loss: 0.4087\n","Epoch 54/100, Train Loss: 0.4181\n","Epoch 55/100, Train Loss: 0.3771\n","Epoch 56/100, Train Loss: 0.4215\n","Epoch 57/100, Train Loss: 0.3733\n","Epoch 58/100, Train Loss: 0.3821\n","Epoch 59/100, Train Loss: 0.4583\n","Epoch 60/100, Train Loss: 0.4164\n","Epoch 61/100, Train Loss: 0.3826\n","Epoch 62/100, Train Loss: 0.3933\n","Epoch 63/100, Train Loss: 0.4086\n","Epoch 64/100, Train Loss: 0.4339\n","Epoch 65/100, Train Loss: 0.4459\n","Epoch 66/100, Train Loss: 0.4277\n","Epoch 67/100, Train Loss: 0.4144\n","Epoch 68/100, Train Loss: 0.4038\n","Epoch 69/100, Train Loss: 0.4107\n","Epoch 70/100, Train Loss: 0.3627\n","Epoch 71/100, Train Loss: 0.3977\n","Epoch 72/100, Train Loss: 0.3842\n","Epoch 73/100, Train Loss: 0.4120\n","Epoch 74/100, Train Loss: 0.4097\n","Epoch 75/100, Train Loss: 0.4226\n","Epoch 76/100, Train Loss: 0.3900\n","Epoch 77/100, Train Loss: 0.4267\n","Epoch 78/100, Train Loss: 0.3923\n","Epoch 79/100, Train Loss: 0.3983\n","Epoch 80/100, Train Loss: 0.4207\n","Epoch 81/100, Train Loss: 0.4378\n","Epoch 82/100, Train Loss: 0.3824\n","Epoch 83/100, Train Loss: 0.3919\n","Epoch 84/100, Train Loss: 0.4389\n","Epoch 85/100, Train Loss: 0.4029\n","Epoch 86/100, Train Loss: 0.3937\n","Epoch 87/100, Train Loss: 0.4328\n","Epoch 88/100, Train Loss: 0.4084\n","Epoch 89/100, Train Loss: 0.3889\n","Epoch 90/100, Train Loss: 0.3962\n","Epoch 91/100, Train Loss: 0.4310\n","Epoch 92/100, Train Loss: 0.4071\n","Epoch 93/100, Train Loss: 0.3828\n","Epoch 94/100, Train Loss: 0.4183\n","Epoch 95/100, Train Loss: 0.3723\n","Epoch 96/100, Train Loss: 0.4102\n","Epoch 97/100, Train Loss: 0.4026\n","Epoch 98/100, Train Loss: 0.3817\n","Epoch 99/100, Train Loss: 0.3885\n","Epoch 100/100, Train Loss: 0.4039\n","Best Validation Loss for 건고추: 0.5050\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"125dd3e635714adaa5268fd838908a05","version_major":2,"version_minor":0},"text/plain":["테스트 파일 추론 중:   0%|          | 0/25 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u003cipython-input-66-63ebe08d55c0\u003e:41: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = GradScaler()\n","\u003cipython-input-58-603e94c8aebc\u003e:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():  # GPU에서 Mixed Precision Training\n","/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/100, Train Loss: 1.0210\n","Epoch 2/100, Train Loss: 0.8367\n","Epoch 3/100, Train Loss: 0.8580\n","Epoch 4/100, Train Loss: 0.7421\n","Epoch 5/100, Train Loss: 0.6737\n","Epoch 6/100, Train Loss: 0.6668\n","Epoch 7/100, Train Loss: 0.6510\n","Epoch 8/100, Train Loss: 0.6746\n","Epoch 9/100, Train Loss: 0.5977\n","Epoch 10/100, Train Loss: 0.5982\n","Epoch 11/100, Train Loss: 0.6301\n","Epoch 12/100, Train Loss: 0.6721\n","Epoch 13/100, Train Loss: 0.6261\n","Epoch 14/100, Train Loss: 0.5799\n","Epoch 15/100, Train Loss: 0.5858\n","Epoch 16/100, Train Loss: 0.6015\n","Epoch 17/100, Train Loss: 0.6075\n","Epoch 18/100, Train Loss: 0.5730\n","Epoch 19/100, Train Loss: 0.5577\n","Epoch 20/100, Train Loss: 0.5736\n","Epoch 21/100, Train Loss: 0.5873\n","Epoch 22/100, Train Loss: 0.5831\n","Epoch 23/100, Train Loss: 0.6036\n","Epoch 24/100, Train Loss: 0.5728\n","Epoch 25/100, Train Loss: 0.5822\n","Epoch 26/100, Train Loss: 0.5397\n","Epoch 27/100, Train Loss: 0.5578\n","Epoch 28/100, Train Loss: 0.5571\n","Epoch 29/100, Train Loss: 0.6150\n","Epoch 30/100, Train Loss: 0.5705\n","Epoch 31/100, Train Loss: 0.5854\n","Epoch 32/100, Train Loss: 0.5930\n","Epoch 33/100, Train Loss: 0.6122\n","Epoch 34/100, Train Loss: 0.5456\n","Epoch 35/100, Train Loss: 0.5761\n","Epoch 36/100, Train Loss: 0.5816\n","Epoch 37/100, Train Loss: 0.5541\n","Epoch 38/100, Train Loss: 0.5393\n","Epoch 39/100, Train Loss: 0.5790\n","Epoch 40/100, Train Loss: 0.5813\n","Epoch 41/100, Train Loss: 0.5595\n","Epoch 42/100, Train Loss: 0.5668\n","Epoch 43/100, Train Loss: 0.5727\n","Epoch 44/100, Train Loss: 0.5861\n","Epoch 45/100, Train Loss: 0.5838\n","Epoch 46/100, Train Loss: 0.5555\n","Epoch 47/100, Train Loss: 0.5428\n","Epoch 48/100, Train Loss: 0.5474\n","Epoch 49/100, Train Loss: 0.5584\n","Epoch 50/100, Train Loss: 0.5344\n","Epoch 51/100, Train Loss: 0.5964\n","Epoch 52/100, Train Loss: 0.5633\n","Epoch 53/100, Train Loss: 0.5593\n","Epoch 54/100, Train Loss: 0.5439\n","Epoch 55/100, Train Loss: 0.5647\n","Epoch 56/100, Train Loss: 0.5499\n","Epoch 57/100, Train Loss: 0.5457\n","Epoch 58/100, Train Loss: 0.5555\n","Epoch 59/100, Train Loss: 0.5425\n","Epoch 60/100, Train Loss: 0.5398\n","Epoch 61/100, Train Loss: 0.5398\n","Epoch 62/100, Train Loss: 0.5669\n","Epoch 63/100, Train Loss: 0.5655\n","Epoch 64/100, Train Loss: 0.5660\n","Epoch 65/100, Train Loss: 0.5502\n","Epoch 66/100, Train Loss: 0.5746\n","Epoch 67/100, Train Loss: 0.5801\n","Epoch 68/100, Train Loss: 0.5538\n","Epoch 69/100, Train Loss: 0.5723\n","Epoch 70/100, Train Loss: 0.5795\n","Epoch 71/100, Train Loss: 0.5598\n","Epoch 72/100, Train Loss: 0.5487\n","Epoch 73/100, Train Loss: 0.5240\n","Epoch 74/100, Train Loss: 0.5486\n","Epoch 75/100, Train Loss: 0.5485\n","Epoch 76/100, Train Loss: 0.5571\n","Epoch 77/100, Train Loss: 0.5436\n","Epoch 78/100, Train Loss: 0.5784\n","Epoch 79/100, Train Loss: 0.5272\n","Epoch 80/100, Train Loss: 0.5695\n","Epoch 81/100, Train Loss: 0.5385\n","Epoch 82/100, Train Loss: 0.5267\n","Epoch 83/100, Train Loss: 0.5552\n","Epoch 84/100, Train Loss: 0.5570\n","Epoch 85/100, Train Loss: 0.5992\n","Epoch 86/100, Train Loss: 0.5217\n","Epoch 87/100, Train Loss: 0.5455\n","Epoch 88/100, Train Loss: 0.5665\n","Epoch 89/100, Train Loss: 0.5578\n","Epoch 90/100, Train Loss: 0.5510\n","Epoch 91/100, Train Loss: 0.5612\n","Epoch 92/100, Train Loss: 0.5463\n","Epoch 93/100, Train Loss: 0.5506\n","Epoch 94/100, Train Loss: 0.5508\n","Epoch 95/100, Train Loss: 0.5591\n","Epoch 96/100, Train Loss: 0.5514\n","Epoch 97/100, Train Loss: 0.5872\n","Epoch 98/100, Train Loss: 0.5696\n","Epoch 99/100, Train Loss: 0.5701\n","Epoch 100/100, Train Loss: 0.5338\n","Best Validation Loss for 사과: 0.4961\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ead76133abc646749ae664a662913518","version_major":2,"version_minor":0},"text/plain":["테스트 파일 추론 중:   0%|          | 0/25 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u003cipython-input-66-63ebe08d55c0\u003e:41: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = GradScaler()\n","\u003cipython-input-58-603e94c8aebc\u003e:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():  # GPU에서 Mixed Precision Training\n","/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/100, Train Loss: 1.4004\n","Epoch 2/100, Train Loss: 1.1647\n","Epoch 3/100, Train Loss: 0.6019\n","Epoch 4/100, Train Loss: 0.5465\n","Epoch 5/100, Train Loss: 0.5408\n","Epoch 6/100, Train Loss: 0.5580\n","Epoch 7/100, Train Loss: 0.5789\n","Epoch 8/100, Train Loss: 0.5467\n","Epoch 9/100, Train Loss: 0.5231\n","Epoch 10/100, Train Loss: 0.5702\n","Epoch 11/100, Train Loss: 0.5178\n","Epoch 12/100, Train Loss: 0.5326\n","Epoch 13/100, Train Loss: 0.5472\n","Epoch 14/100, Train Loss: 0.4913\n","Epoch 15/100, Train Loss: 0.4751\n","Epoch 16/100, Train Loss: 0.5091\n","Epoch 17/100, Train Loss: 0.5055\n","Epoch 18/100, Train Loss: 0.5416\n","Epoch 19/100, Train Loss: 0.4959\n","Epoch 20/100, Train Loss: 0.4895\n","Epoch 21/100, Train Loss: 0.4993\n","Epoch 22/100, Train Loss: 0.4592\n","Epoch 23/100, Train Loss: 0.5408\n","Epoch 24/100, Train Loss: 0.4905\n","Epoch 25/100, Train Loss: 0.4941\n","Epoch 26/100, Train Loss: 0.4752\n","Epoch 27/100, Train Loss: 0.4910\n","Epoch 28/100, Train Loss: 0.4844\n","Epoch 29/100, Train Loss: 0.4890\n","Epoch 30/100, Train Loss: 0.4933\n","Epoch 31/100, Train Loss: 0.4792\n","Epoch 32/100, Train Loss: 0.4822\n","Epoch 33/100, Train Loss: 0.4685\n","Epoch 34/100, Train Loss: 0.4923\n","Epoch 35/100, Train Loss: 0.4710\n","Epoch 36/100, Train Loss: 0.5084\n","Epoch 37/100, Train Loss: 0.4804\n","Epoch 38/100, Train Loss: 0.4809\n","Epoch 39/100, Train Loss: 0.4834\n","Epoch 40/100, Train Loss: 0.4806\n","Epoch 41/100, Train Loss: 0.4730\n","Epoch 42/100, Train Loss: 0.4853\n","Epoch 43/100, Train Loss: 0.4808\n","Epoch 44/100, Train Loss: 0.4994\n","Epoch 45/100, Train Loss: 0.4899\n","Epoch 46/100, Train Loss: 0.5092\n","Epoch 47/100, Train Loss: 0.5033\n","Epoch 48/100, Train Loss: 0.5080\n","Epoch 49/100, Train Loss: 0.4817\n","Epoch 50/100, Train Loss: 0.5097\n","Epoch 51/100, Train Loss: 0.4905\n","Epoch 52/100, Train Loss: 0.4839\n","Epoch 53/100, Train Loss: 0.4815\n","Epoch 54/100, Train Loss: 0.4754\n","Epoch 55/100, Train Loss: 0.4668\n","Epoch 56/100, Train Loss: 0.4854\n","Epoch 57/100, Train Loss: 0.4750\n","Epoch 58/100, Train Loss: 0.4835\n","Epoch 59/100, Train Loss: 0.4732\n","Epoch 60/100, Train Loss: 0.4831\n","Epoch 61/100, Train Loss: 0.4788\n","Epoch 62/100, Train Loss: 0.4696\n","Epoch 63/100, Train Loss: 0.4711\n","Epoch 64/100, Train Loss: 0.4915\n","Epoch 65/100, Train Loss: 0.4476\n","Epoch 66/100, Train Loss: 0.4750\n","Epoch 67/100, Train Loss: 0.4994\n","Epoch 68/100, Train Loss: 0.4794\n","Epoch 69/100, Train Loss: 0.4663\n","Epoch 70/100, Train Loss: 0.4757\n","Epoch 71/100, Train Loss: 0.4905\n","Epoch 72/100, Train Loss: 0.4961\n","Epoch 73/100, Train Loss: 0.4640\n","Epoch 74/100, Train Loss: 0.4743\n","Epoch 75/100, Train Loss: 0.4731\n","Epoch 76/100, Train Loss: 0.4840\n","Epoch 77/100, Train Loss: 0.4805\n","Epoch 78/100, Train Loss: 0.4667\n","Epoch 79/100, Train Loss: 0.5078\n","Epoch 80/100, Train Loss: 0.4923\n","Epoch 81/100, Train Loss: 0.4830\n","Epoch 82/100, Train Loss: 0.4658\n","Epoch 83/100, Train Loss: 0.4770\n","Epoch 84/100, Train Loss: 0.4908\n","Epoch 85/100, Train Loss: 0.4964\n","Epoch 86/100, Train Loss: 0.4834\n","Epoch 87/100, Train Loss: 0.4848\n","Epoch 88/100, Train Loss: 0.4996\n","Epoch 89/100, Train Loss: 0.4756\n","Epoch 90/100, Train Loss: 0.5057\n","Epoch 91/100, Train Loss: 0.4770\n","Epoch 92/100, Train Loss: 0.4746\n","Epoch 93/100, Train Loss: 0.4481\n","Epoch 94/100, Train Loss: 0.5113\n","Epoch 95/100, Train Loss: 0.4946\n","Epoch 96/100, Train Loss: 0.4815\n","Epoch 97/100, Train Loss: 0.4485\n","Epoch 98/100, Train Loss: 0.4927\n","Epoch 99/100, Train Loss: 0.4541\n","Epoch 100/100, Train Loss: 0.4824\n","Best Validation Loss for 감자: 0.5990\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"49b37be89b3c420cb3a3c8925702e18e","version_major":2,"version_minor":0},"text/plain":["테스트 파일 추론 중:   0%|          | 0/25 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u003cipython-input-66-63ebe08d55c0\u003e:41: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = GradScaler()\n","\u003cipython-input-58-603e94c8aebc\u003e:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():  # GPU에서 Mixed Precision Training\n","/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/100, Train Loss: 0.8396\n","Epoch 2/100, Train Loss: 0.4648\n","Epoch 3/100, Train Loss: 0.5513\n","Epoch 4/100, Train Loss: 0.5077\n","Epoch 5/100, Train Loss: 0.4270\n","Epoch 6/100, Train Loss: 0.5018\n","Epoch 7/100, Train Loss: 0.4742\n","Epoch 8/100, Train Loss: 0.4510\n","Epoch 9/100, Train Loss: 0.4345\n","Epoch 10/100, Train Loss: 0.4585\n","Epoch 11/100, Train Loss: 0.4147\n","Epoch 12/100, Train Loss: 0.4211\n","Epoch 13/100, Train Loss: 0.4263\n","Epoch 14/100, Train Loss: 0.4283\n","Epoch 15/100, Train Loss: 0.4389\n","Epoch 16/100, Train Loss: 0.4315\n","Epoch 17/100, Train Loss: 0.4034\n","Epoch 18/100, Train Loss: 0.4004\n","Epoch 19/100, Train Loss: 0.4075\n","Epoch 20/100, Train Loss: 0.4104\n","Epoch 21/100, Train Loss: 0.4279\n","Epoch 22/100, Train Loss: 0.4085\n","Epoch 23/100, Train Loss: 0.3849\n","Epoch 24/100, Train Loss: 0.3951\n","Epoch 25/100, Train Loss: 0.4006\n","Epoch 26/100, Train Loss: 0.3888\n","Epoch 27/100, Train Loss: 0.4238\n","Epoch 28/100, Train Loss: 0.4036\n","Epoch 29/100, Train Loss: 0.3909\n","Epoch 30/100, Train Loss: 0.3875\n","Epoch 31/100, Train Loss: 0.4027\n","Epoch 32/100, Train Loss: 0.3805\n","Epoch 33/100, Train Loss: 0.3981\n","Epoch 34/100, Train Loss: 0.3979\n","Epoch 35/100, Train Loss: 0.3781\n","Epoch 36/100, Train Loss: 0.4077\n","Epoch 37/100, Train Loss: 0.4012\n","Epoch 38/100, Train Loss: 0.3846\n","Epoch 39/100, Train Loss: 0.3993\n","Epoch 40/100, Train Loss: 0.4070\n","Epoch 41/100, Train Loss: 0.3935\n","Epoch 42/100, Train Loss: 0.3895\n","Epoch 43/100, Train Loss: 0.4128\n","Epoch 44/100, Train Loss: 0.4098\n","Epoch 45/100, Train Loss: 0.3904\n","Epoch 46/100, Train Loss: 0.4070\n","Epoch 47/100, Train Loss: 0.4040\n","Epoch 48/100, Train Loss: 0.3776\n","Epoch 49/100, Train Loss: 0.3875\n","Epoch 50/100, Train Loss: 0.4041\n","Epoch 51/100, Train Loss: 0.3626\n","Epoch 52/100, Train Loss: 0.3795\n","Epoch 53/100, Train Loss: 0.3805\n","Epoch 54/100, Train Loss: 0.3833\n","Epoch 55/100, Train Loss: 0.3968\n","Epoch 56/100, Train Loss: 0.3878\n","Epoch 57/100, Train Loss: 0.4114\n","Epoch 58/100, Train Loss: 0.3897\n","Epoch 59/100, Train Loss: 0.4108\n","Epoch 60/100, Train Loss: 0.3786\n","Epoch 61/100, Train Loss: 0.3853\n","Epoch 62/100, Train Loss: 0.3690\n","Epoch 63/100, Train Loss: 0.3605\n","Epoch 64/100, Train Loss: 0.3976\n","Epoch 65/100, Train Loss: 0.3844\n","Epoch 66/100, Train Loss: 0.3845\n","Epoch 67/100, Train Loss: 0.3933\n","Epoch 68/100, Train Loss: 0.3693\n","Epoch 69/100, Train Loss: 0.4008\n","Epoch 70/100, Train Loss: 0.3935\n","Epoch 71/100, Train Loss: 0.3798\n","Epoch 72/100, Train Loss: 0.3765\n","Epoch 73/100, Train Loss: 0.3755\n","Epoch 74/100, Train Loss: 0.3673\n","Epoch 75/100, Train Loss: 0.3654\n","Epoch 76/100, Train Loss: 0.4082\n","Epoch 77/100, Train Loss: 0.3786\n","Epoch 78/100, Train Loss: 0.3925\n","Epoch 79/100, Train Loss: 0.3910\n","Epoch 80/100, Train Loss: 0.3960\n","Epoch 81/100, Train Loss: 0.3867\n","Epoch 82/100, Train Loss: 0.3918\n","Epoch 83/100, Train Loss: 0.3787\n","Epoch 84/100, Train Loss: 0.3714\n","Epoch 85/100, Train Loss: 0.3784\n","Epoch 86/100, Train Loss: 0.3713\n","Epoch 87/100, Train Loss: 0.3731\n","Epoch 88/100, Train Loss: 0.3614\n","Epoch 89/100, Train Loss: 0.3920\n","Epoch 90/100, Train Loss: 0.3845\n","Epoch 91/100, Train Loss: 0.3742\n","Epoch 92/100, Train Loss: 0.3876\n","Epoch 93/100, Train Loss: 0.4081\n","Epoch 94/100, Train Loss: 0.3815\n","Epoch 95/100, Train Loss: 0.3901\n","Epoch 96/100, Train Loss: 0.4007\n","Epoch 97/100, Train Loss: 0.3875\n","Epoch 98/100, Train Loss: 0.3816\n","Epoch 99/100, Train Loss: 0.3767\n","Epoch 100/100, Train Loss: 0.3608\n","Best Validation Loss for 배: 0.4170\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2cb80b684e694a9cb054c8aeb723d3f2","version_major":2,"version_minor":0},"text/plain":["테스트 파일 추론 중:   0%|          | 0/25 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u003cipython-input-66-63ebe08d55c0\u003e:41: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = GradScaler()\n","\u003cipython-input-58-603e94c8aebc\u003e:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():  # GPU에서 Mixed Precision Training\n","/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/100, Train Loss: inf\n","Epoch 2/100, Train Loss: inf\n","Epoch 3/100, Train Loss: inf\n","Epoch 4/100, Train Loss: inf\n","Epoch 5/100, Train Loss: inf\n","Epoch 6/100, Train Loss: inf\n","Epoch 7/100, Train Loss: inf\n","Epoch 8/100, Train Loss: inf\n","Epoch 9/100, Train Loss: inf\n","Epoch 10/100, Train Loss: inf\n","Epoch 11/100, Train Loss: inf\n","Epoch 12/100, Train Loss: inf\n","Epoch 13/100, Train Loss: inf\n","Epoch 14/100, Train Loss: inf\n","Epoch 15/100, Train Loss: inf\n","Epoch 16/100, Train Loss: inf\n","Epoch 17/100, Train Loss: inf\n","Epoch 18/100, Train Loss: inf\n","Epoch 19/100, Train Loss: inf\n","Epoch 20/100, Train Loss: inf\n","Epoch 21/100, Train Loss: inf\n","Epoch 22/100, Train Loss: inf\n","Epoch 23/100, Train Loss: inf\n","Epoch 24/100, Train Loss: inf\n","Epoch 25/100, Train Loss: inf\n","Epoch 26/100, Train Loss: inf\n","Epoch 27/100, Train Loss: inf\n","Epoch 28/100, Train Loss: inf\n","Epoch 29/100, Train Loss: inf\n","Epoch 30/100, Train Loss: inf\n","Epoch 31/100, Train Loss: inf\n","Epoch 32/100, Train Loss: inf\n","Epoch 33/100, Train Loss: inf\n","Epoch 34/100, Train Loss: inf\n","Epoch 35/100, Train Loss: inf\n","Epoch 36/100, Train Loss: inf\n","Epoch 37/100, Train Loss: inf\n","Epoch 38/100, Train Loss: inf\n","Epoch 39/100, Train Loss: inf\n","Epoch 40/100, Train Loss: inf\n","Epoch 41/100, Train Loss: inf\n","Epoch 42/100, Train Loss: inf\n","Epoch 43/100, Train Loss: inf\n","Epoch 44/100, Train Loss: inf\n","Epoch 45/100, Train Loss: inf\n","Epoch 46/100, Train Loss: inf\n","Epoch 47/100, Train Loss: inf\n","Epoch 48/100, Train Loss: inf\n","Epoch 49/100, Train Loss: inf\n","Epoch 50/100, Train Loss: inf\n","Epoch 51/100, Train Loss: inf\n","Epoch 52/100, Train Loss: inf\n","Epoch 53/100, Train Loss: inf\n","Epoch 54/100, Train Loss: inf\n","Epoch 55/100, Train Loss: inf\n","Epoch 56/100, Train Loss: inf\n","Epoch 57/100, Train Loss: inf\n","Epoch 58/100, Train Loss: inf\n","Epoch 59/100, Train Loss: inf\n","Epoch 60/100, Train Loss: inf\n","Epoch 61/100, Train Loss: inf\n","Epoch 62/100, Train Loss: inf\n","Epoch 63/100, Train Loss: inf\n","Epoch 64/100, Train Loss: inf\n","Epoch 65/100, Train Loss: inf\n","Epoch 66/100, Train Loss: inf\n","Epoch 67/100, Train Loss: inf\n","Epoch 68/100, Train Loss: inf\n","Epoch 69/100, Train Loss: inf\n","Epoch 70/100, Train Loss: inf\n","Epoch 71/100, Train Loss: inf\n","Epoch 72/100, Train Loss: inf\n","Epoch 73/100, Train Loss: inf\n","Epoch 74/100, Train Loss: inf\n","Epoch 75/100, Train Loss: inf\n","Epoch 76/100, Train Loss: inf\n","Epoch 77/100, Train Loss: inf\n","Epoch 78/100, Train Loss: inf\n","Epoch 79/100, Train Loss: inf\n","Epoch 80/100, Train Loss: inf\n","Epoch 81/100, Train Loss: inf\n","Epoch 82/100, Train Loss: inf\n","Epoch 83/100, Train Loss: inf\n","Epoch 84/100, Train Loss: inf\n","Epoch 85/100, Train Loss: inf\n","Epoch 86/100, Train Loss: inf\n","Epoch 87/100, Train Loss: inf\n","Epoch 88/100, Train Loss: inf\n","Epoch 89/100, Train Loss: inf\n","Epoch 90/100, Train Loss: inf\n","Epoch 91/100, Train Loss: inf\n","Epoch 92/100, Train Loss: inf\n","Epoch 93/100, Train Loss: inf\n","Epoch 94/100, Train Loss: inf\n","Epoch 95/100, Train Loss: inf\n","Epoch 96/100, Train Loss: inf\n","Epoch 97/100, Train Loss: inf\n","Epoch 98/100, Train Loss: inf\n","Epoch 99/100, Train Loss: inf\n","Epoch 100/100, Train Loss: inf\n","Best Validation Loss for 깐마늘(국산): inf\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"30a8fad335904b07abd4ecc710935f6f","version_major":2,"version_minor":0},"text/plain":["테스트 파일 추론 중:   0%|          | 0/25 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u003cipython-input-66-63ebe08d55c0\u003e:41: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = GradScaler()\n","\u003cipython-input-58-603e94c8aebc\u003e:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():  # GPU에서 Mixed Precision Training\n","/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/100, Train Loss: inf\n","Epoch 2/100, Train Loss: inf\n","Epoch 3/100, Train Loss: inf\n","Epoch 4/100, Train Loss: inf\n","Epoch 5/100, Train Loss: inf\n","Epoch 6/100, Train Loss: inf\n","Epoch 7/100, Train Loss: inf\n","Epoch 8/100, Train Loss: inf\n","Epoch 9/100, Train Loss: inf\n","Epoch 10/100, Train Loss: inf\n","Epoch 11/100, Train Loss: inf\n","Epoch 12/100, Train Loss: inf\n","Epoch 13/100, Train Loss: inf\n","Epoch 14/100, Train Loss: inf\n","Epoch 15/100, Train Loss: inf\n","Epoch 16/100, Train Loss: inf\n","Epoch 17/100, Train Loss: inf\n","Epoch 18/100, Train Loss: inf\n","Epoch 19/100, Train Loss: inf\n","Epoch 20/100, Train Loss: inf\n","Epoch 21/100, Train Loss: inf\n","Epoch 22/100, Train Loss: inf\n","Epoch 23/100, Train Loss: inf\n","Epoch 24/100, Train Loss: inf\n","Epoch 25/100, Train Loss: inf\n","Epoch 26/100, Train Loss: inf\n","Epoch 27/100, Train Loss: inf\n","Epoch 28/100, Train Loss: inf\n","Epoch 29/100, Train Loss: inf\n","Epoch 30/100, Train Loss: inf\n","Epoch 31/100, Train Loss: inf\n","Epoch 32/100, Train Loss: inf\n","Epoch 33/100, Train Loss: inf\n","Epoch 34/100, Train Loss: inf\n","Epoch 35/100, Train Loss: inf\n","Epoch 36/100, Train Loss: inf\n","Epoch 37/100, Train Loss: inf\n","Epoch 38/100, Train Loss: inf\n","Epoch 39/100, Train Loss: inf\n","Epoch 40/100, Train Loss: inf\n","Epoch 41/100, Train Loss: inf\n","Epoch 42/100, Train Loss: inf\n","Epoch 43/100, Train Loss: inf\n","Epoch 44/100, Train Loss: inf\n","Epoch 45/100, Train Loss: inf\n","Epoch 46/100, Train Loss: inf\n","Epoch 47/100, Train Loss: inf\n","Epoch 48/100, Train Loss: inf\n","Epoch 49/100, Train Loss: inf\n","Epoch 50/100, Train Loss: inf\n","Epoch 51/100, Train Loss: inf\n","Epoch 52/100, Train Loss: inf\n","Epoch 53/100, Train Loss: inf\n","Epoch 54/100, Train Loss: inf\n","Epoch 55/100, Train Loss: inf\n","Epoch 56/100, Train Loss: inf\n","Epoch 57/100, Train Loss: inf\n","Epoch 58/100, Train Loss: inf\n","Epoch 59/100, Train Loss: inf\n","Epoch 60/100, Train Loss: inf\n","Epoch 61/100, Train Loss: inf\n","Epoch 62/100, Train Loss: inf\n","Epoch 63/100, Train Loss: inf\n","Epoch 64/100, Train Loss: inf\n","Epoch 65/100, Train Loss: inf\n","Epoch 66/100, Train Loss: inf\n","Epoch 67/100, Train Loss: inf\n","Epoch 68/100, Train Loss: inf\n","Epoch 69/100, Train Loss: inf\n","Epoch 70/100, Train Loss: inf\n","Epoch 71/100, Train Loss: inf\n","Epoch 72/100, Train Loss: inf\n","Epoch 73/100, Train Loss: inf\n","Epoch 74/100, Train Loss: inf\n","Epoch 75/100, Train Loss: inf\n","Epoch 76/100, Train Loss: inf\n","Epoch 77/100, Train Loss: inf\n","Epoch 78/100, Train Loss: inf\n","Epoch 79/100, Train Loss: inf\n","Epoch 80/100, Train Loss: inf\n","Epoch 81/100, Train Loss: inf\n","Epoch 82/100, Train Loss: inf\n","Epoch 83/100, Train Loss: inf\n","Epoch 84/100, Train Loss: inf\n","Epoch 85/100, Train Loss: inf\n","Epoch 86/100, Train Loss: inf\n","Epoch 87/100, Train Loss: inf\n","Epoch 88/100, Train Loss: inf\n","Epoch 89/100, Train Loss: inf\n","Epoch 90/100, Train Loss: inf\n","Epoch 91/100, Train Loss: inf\n","Epoch 92/100, Train Loss: inf\n","Epoch 93/100, Train Loss: inf\n","Epoch 94/100, Train Loss: inf\n","Epoch 95/100, Train Loss: inf\n","Epoch 96/100, Train Loss: inf\n","Epoch 97/100, Train Loss: inf\n","Epoch 98/100, Train Loss: inf\n","Epoch 99/100, Train Loss: inf\n","Epoch 100/100, Train Loss: inf\n","Best Validation Loss for 무: inf\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"45978aeff2e945c69f25fc05d311402d","version_major":2,"version_minor":0},"text/plain":["테스트 파일 추론 중:   0%|          | 0/25 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u003cipython-input-66-63ebe08d55c0\u003e:41: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = GradScaler()\n","\u003cipython-input-58-603e94c8aebc\u003e:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():  # GPU에서 Mixed Precision Training\n","/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/100, Train Loss: 1.1001\n","Epoch 2/100, Train Loss: 0.9389\n","Epoch 3/100, Train Loss: 0.9111\n","Epoch 4/100, Train Loss: 0.7768\n","Epoch 5/100, Train Loss: 0.7258\n","Epoch 6/100, Train Loss: 0.6590\n","Epoch 7/100, Train Loss: 0.6802\n","Epoch 8/100, Train Loss: 0.6395\n","Epoch 9/100, Train Loss: 0.6569\n","Epoch 10/100, Train Loss: 0.6775\n","Epoch 11/100, Train Loss: 0.6629\n","Epoch 12/100, Train Loss: 0.6821\n","Epoch 13/100, Train Loss: 0.6882\n","Epoch 14/100, Train Loss: 0.6926\n","Epoch 15/100, Train Loss: 0.6557\n","Epoch 16/100, Train Loss: 0.6685\n","Epoch 17/100, Train Loss: 0.6511\n","Epoch 18/100, Train Loss: 0.6528\n","Epoch 19/100, Train Loss: 0.6754\n","Epoch 20/100, Train Loss: 0.6530\n","Epoch 21/100, Train Loss: 0.6897\n","Epoch 22/100, Train Loss: 0.6503\n","Epoch 23/100, Train Loss: 0.6333\n","Epoch 24/100, Train Loss: 0.6406\n","Epoch 25/100, Train Loss: 0.6253\n","Epoch 26/100, Train Loss: 0.6585\n","Epoch 27/100, Train Loss: 0.6384\n","Epoch 28/100, Train Loss: 0.6859\n","Epoch 29/100, Train Loss: 0.6576\n","Epoch 30/100, Train Loss: 0.6770\n","Epoch 31/100, Train Loss: 0.6434\n","Epoch 32/100, Train Loss: 0.6780\n","Epoch 33/100, Train Loss: 0.6422\n","Epoch 34/100, Train Loss: 0.6908\n","Epoch 35/100, Train Loss: 0.6442\n","Epoch 36/100, Train Loss: 0.6422\n","Epoch 37/100, Train Loss: 0.6340\n","Epoch 38/100, Train Loss: 0.6400\n","Epoch 39/100, Train Loss: 0.6431\n","Epoch 40/100, Train Loss: 0.6755\n","Epoch 41/100, Train Loss: 0.6596\n","Epoch 42/100, Train Loss: 0.6552\n","Epoch 43/100, Train Loss: 0.6670\n","Epoch 44/100, Train Loss: 0.6467\n","Epoch 45/100, Train Loss: 0.6251\n","Epoch 46/100, Train Loss: 0.6441\n","Epoch 47/100, Train Loss: 0.6590\n","Epoch 48/100, Train Loss: 0.6672\n","Epoch 49/100, Train Loss: 0.6411\n","Epoch 50/100, Train Loss: 0.6521\n","Epoch 51/100, Train Loss: 0.6492\n","Epoch 52/100, Train Loss: 0.6497\n","Epoch 53/100, Train Loss: 0.6444\n","Epoch 54/100, Train Loss: 0.6547\n","Epoch 55/100, Train Loss: 0.6403\n","Epoch 56/100, Train Loss: 0.6429\n","Epoch 57/100, Train Loss: 0.6367\n","Epoch 58/100, Train Loss: 0.6532\n","Epoch 59/100, Train Loss: 0.6459\n","Epoch 60/100, Train Loss: 0.6457\n","Epoch 61/100, Train Loss: 0.6477\n","Epoch 62/100, Train Loss: 0.6573\n","Epoch 63/100, Train Loss: 0.6430\n","Epoch 64/100, Train Loss: 0.6338\n","Epoch 65/100, Train Loss: 0.6318\n","Epoch 66/100, Train Loss: 0.6524\n","Epoch 67/100, Train Loss: 0.6619\n","Epoch 68/100, Train Loss: 0.6137\n","Epoch 69/100, Train Loss: 0.6201\n","Epoch 70/100, Train Loss: 0.6369\n","Epoch 71/100, Train Loss: 0.6515\n","Epoch 72/100, Train Loss: 0.6348\n","Epoch 73/100, Train Loss: 0.6466\n","Epoch 74/100, Train Loss: 0.6307\n","Epoch 75/100, Train Loss: 0.6099\n","Epoch 76/100, Train Loss: 0.6597\n","Epoch 77/100, Train Loss: 0.6483\n","Epoch 78/100, Train Loss: 0.6558\n","Epoch 79/100, Train Loss: 0.6336\n","Epoch 80/100, Train Loss: 0.6381\n","Epoch 81/100, Train Loss: 0.6325\n","Epoch 82/100, Train Loss: 0.6199\n","Epoch 83/100, Train Loss: 0.6541\n","Epoch 84/100, Train Loss: 0.6574\n","Epoch 85/100, Train Loss: 0.6331\n","Epoch 86/100, Train Loss: 0.6416\n","Epoch 87/100, Train Loss: 0.6480\n","Epoch 88/100, Train Loss: 0.6483\n","Epoch 89/100, Train Loss: 0.6496\n","Epoch 90/100, Train Loss: 0.6249\n","Epoch 91/100, Train Loss: 0.6269\n","Epoch 92/100, Train Loss: 0.6269\n","Epoch 93/100, Train Loss: 0.6631\n","Epoch 94/100, Train Loss: 0.6602\n","Epoch 95/100, Train Loss: 0.6724\n","Epoch 96/100, Train Loss: 0.6426\n","Epoch 97/100, Train Loss: 0.6728\n","Epoch 98/100, Train Loss: 0.6296\n","Epoch 99/100, Train Loss: 0.6227\n","Epoch 100/100, Train Loss: 0.6472\n","Best Validation Loss for 상추: 0.5991\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"13d94698b9db4b09af175295d6392c6e","version_major":2,"version_minor":0},"text/plain":["테스트 파일 추론 중:   0%|          | 0/25 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u003cipython-input-66-63ebe08d55c0\u003e:41: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = GradScaler()\n","\u003cipython-input-58-603e94c8aebc\u003e:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():  # GPU에서 Mixed Precision Training\n","/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/100, Train Loss: 0.6997\n","Epoch 2/100, Train Loss: 0.7969\n","Epoch 3/100, Train Loss: 0.6848\n","Epoch 4/100, Train Loss: 0.6458\n","Epoch 5/100, Train Loss: 0.5735\n","Epoch 6/100, Train Loss: 0.6371\n","Epoch 7/100, Train Loss: 0.6100\n","Epoch 8/100, Train Loss: 0.6031\n","Epoch 9/100, Train Loss: 0.5529\n","Epoch 10/100, Train Loss: 0.5602\n","Epoch 11/100, Train Loss: 0.5663\n","Epoch 12/100, Train Loss: 0.5248\n","Epoch 13/100, Train Loss: 0.5595\n","Epoch 14/100, Train Loss: 0.5878\n","Epoch 15/100, Train Loss: 0.5522\n","Epoch 16/100, Train Loss: 0.5655\n","Epoch 17/100, Train Loss: 0.5615\n","Epoch 18/100, Train Loss: 0.5461\n","Epoch 19/100, Train Loss: 0.5509\n","Epoch 20/100, Train Loss: 0.5484\n","Epoch 21/100, Train Loss: 0.5568\n","Epoch 22/100, Train Loss: 0.5394\n","Epoch 23/100, Train Loss: 0.5568\n","Epoch 24/100, Train Loss: 0.5331\n","Epoch 25/100, Train Loss: 0.5522\n","Epoch 26/100, Train Loss: 0.5434\n","Epoch 27/100, Train Loss: 0.5250\n","Epoch 28/100, Train Loss: 0.5446\n","Epoch 29/100, Train Loss: 0.5453\n","Epoch 30/100, Train Loss: 0.5239\n","Epoch 31/100, Train Loss: 0.5317\n","Epoch 32/100, Train Loss: 0.5354\n","Epoch 33/100, Train Loss: 0.5460\n","Epoch 34/100, Train Loss: 0.5523\n","Epoch 35/100, Train Loss: 0.5210\n","Epoch 36/100, Train Loss: 0.5366\n","Epoch 37/100, Train Loss: 0.5437\n","Epoch 38/100, Train Loss: 0.5034\n","Epoch 39/100, Train Loss: 0.5382\n","Epoch 40/100, Train Loss: 0.5371\n","Epoch 41/100, Train Loss: 0.5527\n","Epoch 42/100, Train Loss: 0.5262\n","Epoch 43/100, Train Loss: 0.5027\n","Epoch 44/100, Train Loss: 0.5717\n","Epoch 45/100, Train Loss: 0.5301\n","Epoch 46/100, Train Loss: 0.5211\n","Epoch 47/100, Train Loss: 0.5451\n","Epoch 48/100, Train Loss: 0.5181\n","Epoch 49/100, Train Loss: 0.5354\n","Epoch 50/100, Train Loss: 0.5069\n","Epoch 51/100, Train Loss: 0.5268\n","Epoch 52/100, Train Loss: 0.5399\n","Epoch 53/100, Train Loss: 0.5339\n","Epoch 54/100, Train Loss: 0.5326\n","Epoch 55/100, Train Loss: 0.5321\n","Epoch 56/100, Train Loss: 0.5318\n","Epoch 57/100, Train Loss: 0.5193\n","Epoch 58/100, Train Loss: 0.5420\n","Epoch 59/100, Train Loss: 0.5344\n","Epoch 60/100, Train Loss: 0.5595\n","Epoch 61/100, Train Loss: 0.5341\n","Epoch 62/100, Train Loss: 0.5447\n","Epoch 63/100, Train Loss: 0.5240\n","Epoch 64/100, Train Loss: 0.5302\n","Epoch 65/100, Train Loss: 0.5280\n","Epoch 66/100, Train Loss: 0.5250\n","Epoch 67/100, Train Loss: 0.5132\n","Epoch 68/100, Train Loss: 0.5153\n","Epoch 69/100, Train Loss: 0.5382\n","Epoch 70/100, Train Loss: 0.5339\n","Epoch 71/100, Train Loss: 0.5455\n","Epoch 72/100, Train Loss: 0.5293\n","Epoch 73/100, Train Loss: 0.5391\n","Epoch 74/100, Train Loss: 0.5234\n","Epoch 75/100, Train Loss: 0.5352\n","Epoch 76/100, Train Loss: 0.5455\n","Epoch 77/100, Train Loss: 0.5211\n","Epoch 78/100, Train Loss: 0.5248\n","Epoch 79/100, Train Loss: 0.5302\n","Epoch 80/100, Train Loss: 0.5141\n","Epoch 81/100, Train Loss: 0.5236\n","Epoch 82/100, Train Loss: 0.5261\n","Epoch 83/100, Train Loss: 0.5268\n","Epoch 84/100, Train Loss: 0.5351\n","Epoch 85/100, Train Loss: 0.5316\n","Epoch 86/100, Train Loss: 0.5150\n","Epoch 87/100, Train Loss: 0.5241\n","Epoch 88/100, Train Loss: 0.5364\n","Epoch 89/100, Train Loss: 0.5325\n","Epoch 90/100, Train Loss: 0.5462\n","Epoch 91/100, Train Loss: 0.5056\n","Epoch 92/100, Train Loss: 0.5139\n","Epoch 93/100, Train Loss: 0.5064\n","Epoch 94/100, Train Loss: 0.5175\n","Epoch 95/100, Train Loss: 0.5215\n","Epoch 96/100, Train Loss: 0.5180\n","Epoch 97/100, Train Loss: 0.5330\n","Epoch 98/100, Train Loss: 0.5396\n","Epoch 99/100, Train Loss: 0.5180\n","Epoch 100/100, Train Loss: 0.5222\n","Best Validation Loss for 배추: 0.5214\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0608c2d7b2194942a7b6fea1872ddf1c","version_major":2,"version_minor":0},"text/plain":["테스트 파일 추론 중:   0%|          | 0/25 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u003cipython-input-66-63ebe08d55c0\u003e:41: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = GradScaler()\n","\u003cipython-input-58-603e94c8aebc\u003e:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():  # GPU에서 Mixed Precision Training\n","/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/100, Train Loss: 1.0239\n","Epoch 2/100, Train Loss: 0.4877\n","Epoch 3/100, Train Loss: 0.5156\n","Epoch 4/100, Train Loss: 0.5013\n","Epoch 5/100, Train Loss: 0.3423\n","Epoch 6/100, Train Loss: 0.4015\n","Epoch 7/100, Train Loss: 0.3876\n","Epoch 8/100, Train Loss: 0.3391\n","Epoch 9/100, Train Loss: 0.3638\n","Epoch 10/100, Train Loss: 0.3024\n","Epoch 11/100, Train Loss: 0.3080\n","Epoch 12/100, Train Loss: 0.3229\n","Epoch 13/100, Train Loss: 0.3018\n","Epoch 14/100, Train Loss: 0.3154\n","Epoch 15/100, Train Loss: 0.3123\n","Epoch 16/100, Train Loss: 0.3108\n","Epoch 17/100, Train Loss: 0.3124\n","Epoch 18/100, Train Loss: 0.2999\n","Epoch 19/100, Train Loss: 0.3152\n","Epoch 20/100, Train Loss: 0.2932\n","Epoch 21/100, Train Loss: 0.3062\n","Epoch 22/100, Train Loss: 0.2751\n","Epoch 23/100, Train Loss: 0.2883\n","Epoch 24/100, Train Loss: 0.2919\n","Epoch 25/100, Train Loss: 0.3183\n","Epoch 26/100, Train Loss: 0.3109\n","Epoch 27/100, Train Loss: 0.2881\n","Epoch 28/100, Train Loss: 0.2956\n","Epoch 29/100, Train Loss: 0.2929\n","Epoch 30/100, Train Loss: 0.3038\n","Epoch 31/100, Train Loss: 0.2746\n","Epoch 32/100, Train Loss: 0.2943\n","Epoch 33/100, Train Loss: 0.2835\n","Epoch 34/100, Train Loss: 0.2879\n","Epoch 35/100, Train Loss: 0.3089\n","Epoch 36/100, Train Loss: 0.2815\n","Epoch 37/100, Train Loss: 0.3076\n","Epoch 38/100, Train Loss: 0.2823\n","Epoch 39/100, Train Loss: 0.2890\n","Epoch 40/100, Train Loss: 0.2732\n","Epoch 41/100, Train Loss: 0.2818\n","Epoch 42/100, Train Loss: 0.2708\n","Epoch 43/100, Train Loss: 0.3007\n","Epoch 44/100, Train Loss: 0.2841\n","Epoch 45/100, Train Loss: 0.2744\n","Epoch 46/100, Train Loss: 0.2872\n","Epoch 47/100, Train Loss: 0.2804\n","Epoch 48/100, Train Loss: 0.2819\n","Epoch 49/100, Train Loss: 0.2917\n","Epoch 50/100, Train Loss: 0.2706\n","Epoch 51/100, Train Loss: 0.2765\n","Epoch 52/100, Train Loss: 0.3032\n","Epoch 53/100, Train Loss: 0.2960\n","Epoch 54/100, Train Loss: 0.2901\n","Epoch 55/100, Train Loss: 0.2990\n","Epoch 56/100, Train Loss: 0.2800\n","Epoch 57/100, Train Loss: 0.2969\n","Epoch 58/100, Train Loss: 0.2747\n","Epoch 59/100, Train Loss: 0.2821\n","Epoch 60/100, Train Loss: 0.2958\n","Epoch 61/100, Train Loss: 0.2763\n","Epoch 62/100, Train Loss: 0.3006\n","Epoch 63/100, Train Loss: 0.2790\n","Epoch 64/100, Train Loss: 0.2866\n","Epoch 65/100, Train Loss: 0.2780\n","Epoch 66/100, Train Loss: 0.2822\n","Epoch 67/100, Train Loss: 0.2841\n","Epoch 68/100, Train Loss: 0.2980\n","Epoch 69/100, Train Loss: 0.2960\n","Epoch 70/100, Train Loss: 0.2830\n","Epoch 71/100, Train Loss: 0.2844\n","Epoch 72/100, Train Loss: 0.3035\n","Epoch 73/100, Train Loss: 0.2897\n","Epoch 74/100, Train Loss: 0.2943\n","Epoch 75/100, Train Loss: 0.2783\n","Epoch 76/100, Train Loss: 0.2885\n","Epoch 77/100, Train Loss: 0.2752\n","Epoch 78/100, Train Loss: 0.2930\n","Epoch 79/100, Train Loss: 0.2957\n","Epoch 80/100, Train Loss: 0.2767\n","Epoch 81/100, Train Loss: 0.2717\n","Epoch 82/100, Train Loss: 0.2917\n","Epoch 83/100, Train Loss: 0.2728\n","Epoch 84/100, Train Loss: 0.2645\n","Epoch 85/100, Train Loss: 0.2877\n","Epoch 86/100, Train Loss: 0.2889\n","Epoch 87/100, Train Loss: 0.2790\n","Epoch 88/100, Train Loss: 0.2647\n","Epoch 89/100, Train Loss: 0.2715\n","Epoch 90/100, Train Loss: 0.2706\n","Epoch 91/100, Train Loss: 0.2741\n","Epoch 92/100, Train Loss: 0.2598\n","Epoch 93/100, Train Loss: 0.2960\n","Epoch 94/100, Train Loss: 0.2889\n","Epoch 95/100, Train Loss: 0.2843\n","Epoch 96/100, Train Loss: 0.2967\n","Epoch 97/100, Train Loss: 0.2828\n","Epoch 98/100, Train Loss: 0.2835\n","Epoch 99/100, Train Loss: 0.2773\n","Epoch 100/100, Train Loss: 0.2900\n","Best Validation Loss for 양파: 0.2608\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e7af12e75b0143c98f903dc5af0ccc1d","version_major":2,"version_minor":0},"text/plain":["테스트 파일 추론 중:   0%|          | 0/25 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u003cipython-input-55-b163921a813c\u003e:102: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  filtered_data[numeric_columns] = filtered_data[numeric_columns].fillna(0)\n","\u003cipython-input-55-b163921a813c\u003e:105: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  filtered_data[numeric_columns] = filtered_data[numeric_columns].interpolate(method='linear', limit_direction='both')\n","\u003cipython-input-66-63ebe08d55c0\u003e:41: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = GradScaler()\n","\u003cipython-input-58-603e94c8aebc\u003e:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():  # GPU에서 Mixed Precision Training\n","/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/100, Train Loss: 0.9420\n","Epoch 2/100, Train Loss: 0.6839\n","Epoch 3/100, Train Loss: 0.4462\n","Epoch 4/100, Train Loss: 0.4965\n","Epoch 5/100, Train Loss: 0.4028\n","Epoch 6/100, Train Loss: 0.3905\n","Epoch 7/100, Train Loss: 0.4279\n","Epoch 8/100, Train Loss: 0.4146\n","Epoch 9/100, Train Loss: 0.4314\n","Epoch 10/100, Train Loss: 0.3550\n","Epoch 11/100, Train Loss: 0.3824\n","Epoch 12/100, Train Loss: 0.3534\n","Epoch 13/100, Train Loss: 0.3787\n","Epoch 14/100, Train Loss: 0.3895\n","Epoch 15/100, Train Loss: 0.3642\n","Epoch 16/100, Train Loss: 0.3333\n","Epoch 17/100, Train Loss: 0.3641\n","Epoch 18/100, Train Loss: 0.3257\n","Epoch 19/100, Train Loss: 0.3702\n","Epoch 20/100, Train Loss: 0.3750\n","Epoch 21/100, Train Loss: 0.3562\n","Epoch 22/100, Train Loss: 0.3386\n","Epoch 23/100, Train Loss: 0.3684\n","Epoch 24/100, Train Loss: 0.3518\n","Epoch 25/100, Train Loss: 0.3529\n","Epoch 26/100, Train Loss: 0.3467\n","Epoch 27/100, Train Loss: 0.3419\n","Epoch 28/100, Train Loss: 0.3487\n","Epoch 29/100, Train Loss: 0.3717\n","Epoch 30/100, Train Loss: 0.3427\n","Epoch 31/100, Train Loss: 0.3479\n","Epoch 32/100, Train Loss: 0.3540\n","Epoch 33/100, Train Loss: 0.3404\n","Epoch 34/100, Train Loss: 0.3506\n","Epoch 35/100, Train Loss: 0.3337\n","Epoch 36/100, Train Loss: 0.3656\n","Epoch 37/100, Train Loss: 0.3513\n","Epoch 38/100, Train Loss: 0.3499\n","Epoch 39/100, Train Loss: 0.3564\n","Epoch 40/100, Train Loss: 0.3501\n","Epoch 41/100, Train Loss: 0.3471\n","Epoch 42/100, Train Loss: 0.3402\n","Epoch 43/100, Train Loss: 0.3483\n","Epoch 44/100, Train Loss: 0.3250\n","Epoch 45/100, Train Loss: 0.3399\n","Epoch 46/100, Train Loss: 0.3466\n","Epoch 47/100, Train Loss: 0.3285\n","Epoch 48/100, Train Loss: 0.3446\n","Epoch 49/100, Train Loss: 0.3413\n","Epoch 50/100, Train Loss: 0.3680\n","Epoch 51/100, Train Loss: 0.3468\n","Epoch 52/100, Train Loss: 0.3322\n","Epoch 53/100, Train Loss: 0.3682\n","Epoch 54/100, Train Loss: 0.3480\n","Epoch 55/100, Train Loss: 0.3268\n","Epoch 56/100, Train Loss: 0.3411\n","Epoch 57/100, Train Loss: 0.3596\n","Epoch 58/100, Train Loss: 0.3505\n","Epoch 59/100, Train Loss: 0.3425\n","Epoch 60/100, Train Loss: 0.3361\n","Epoch 61/100, Train Loss: 0.3405\n","Epoch 62/100, Train Loss: 0.3547\n","Epoch 63/100, Train Loss: 0.3312\n","Epoch 64/100, Train Loss: 0.3531\n","Epoch 65/100, Train Loss: 0.3488\n","Epoch 66/100, Train Loss: 0.3503\n","Epoch 67/100, Train Loss: 0.3454\n","Epoch 68/100, Train Loss: 0.3271\n","Epoch 69/100, Train Loss: 0.3525\n","Epoch 70/100, Train Loss: 0.3341\n","Epoch 71/100, Train Loss: 0.3165\n","Epoch 72/100, Train Loss: 0.3189\n","Epoch 73/100, Train Loss: 0.3637\n","Epoch 74/100, Train Loss: 0.3413\n","Epoch 75/100, Train Loss: 0.3407\n","Epoch 76/100, Train Loss: 0.3270\n","Epoch 77/100, Train Loss: 0.3208\n","Epoch 78/100, Train Loss: 0.3419\n","Epoch 79/100, Train Loss: 0.3441\n","Epoch 80/100, Train Loss: 0.3462\n","Epoch 81/100, Train Loss: 0.3261\n","Epoch 82/100, Train Loss: 0.3339\n","Epoch 83/100, Train Loss: 0.3407\n","Epoch 84/100, Train Loss: 0.3299\n","Epoch 85/100, Train Loss: 0.3452\n","Epoch 86/100, Train Loss: 0.3423\n","Epoch 87/100, Train Loss: 0.3499\n","Epoch 88/100, Train Loss: 0.3364\n","Epoch 89/100, Train Loss: 0.3276\n","Epoch 90/100, Train Loss: 0.3171\n","Epoch 91/100, Train Loss: 0.3054\n","Epoch 92/100, Train Loss: 0.3504\n","Epoch 93/100, Train Loss: 0.3207\n","Epoch 94/100, Train Loss: 0.3379\n","Epoch 95/100, Train Loss: 0.3427\n","Epoch 96/100, Train Loss: 0.3228\n","Epoch 97/100, Train Loss: 0.3316\n","Epoch 98/100, Train Loss: 0.3346\n","Epoch 99/100, Train Loss: 0.3250\n","Epoch 100/100, Train Loss: 0.3338\n","Best Validation Loss for 대파: 0.3879\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"86adfc40acce4888aacfcf856f74c6f7","version_major":2,"version_minor":0},"text/plain":["테스트 파일 추론 중:   0%|          | 0/25 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from torch.cuda.amp import GradScaler\n","from torch.cuda.amp import autocast\n","\n","\n","품목별_predictions = {}\n","품목별_scalers = {}\n","\n","pbar_outer = tqdm(item_list, desc=\"품목 처리 중\", position=0)\n","for 품목명 in pbar_outer:\n","    pbar_outer.set_description(f\"품목별 전처리 및 모델 학습 -\u003e {품목명}\")\n","    train_data, scaler = process_data(\"./train/train.csv\",\n","                              \"./train/meta/TRAIN_산지공판장_2018-2021.csv\",\n","                              \"./train/meta/TRAIN_전국도매_2018-2021.csv\",\n","                              품목명)\n","    품목별_scalers[품목명] = scaler\n","    dataset = AgriculturePriceDataset(train_data)\n","\n","    # 데이터를 train과 validation으로 분할\n","    train_data, val_data = train_test_split(dataset, test_size=0.2, random_state=42)\n","\n","    train_loader = DataLoader(train_data, CFG.batch_size, shuffle=True)\n","    val_loader = DataLoader(val_data, CFG.batch_size, shuffle=False)\n","\n","    input_size = len(dataset.numeric_columns)\n","\n","    model = TimeSeriesTransformer(\n","            input_size=input_size,\n","            hidden_size=CFG.hidden_size,\n","            num_layers=CFG.num_layers,\n","            num_heads=CFG.num_heads,\n","            output_size=CFG.output_size,\n","            dropout=CFG.dropout\n","        ).to(device)\n","    criterion = NMAELoss()\n","    optimizer = torch.optim.Adam(model.parameters(), CFG.learning_rate)\n","\n","    best_val_loss = float('inf')\n","    os.makedirs('models', exist_ok=True)\n","\n","    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=CFG.step_size, gamma=CFG.gamma)\n","    scaler = GradScaler()\n","\n","    for epoch in range(CFG.epoch):\n","        train_loss = train_model(\n","            model, train_loader, criterion, optimizer, scheduler, scaler, device\n","        )\n","        val_loss = evaluate_model(model, val_loader, criterion, device)\n","        # Gradient clipping\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","        if val_loss \u003c best_val_loss:\n","            best_val_loss = val_loss\n","            torch.save(model.state_dict(), f'models/best_model_{품목명}.pth')\n","\n","        print(f'Epoch {epoch+1}/{CFG.epoch}, Train Loss: {train_loss:.4f}')\n","    print(f'Best Validation Loss for {품목명}: {best_val_loss:.4f}')\n","\n","    품목_predictions = []\n","\n","    ### 추론\n","    pbar_inner = tqdm(range(25), desc=\"테스트 파일 추론 중\", position=1, leave=False)\n","    for i in pbar_inner:\n","        test_file = f\"./test/TEST_{i:02d}.csv\"\n","        산지공판장_file = f\"./test/meta/TEST_산지공판장_{i:02d}.csv\"\n","        전국도매_file = f\"./test/meta/TEST_전국도매_{i:02d}.csv\"\n","\n","        test_data, _ = process_data(test_file, 산지공판장_file, 전국도매_file, 품목명, scaler=품목별_scalers[품목명])\n","        test_dataset = AgriculturePriceDataset(test_data, is_test=True)\n","        test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n","\n","        model.eval()\n","        predictions = []\n","        with torch.no_grad():\n","            for batch in test_loader:\n","                batch = batch.to(device)  # 입력 데이터를 GPU로 이동\n","                output = model(batch)    # 모델과 데이터가 GPU에서 처리\n","                predictions.append(output.cpu().numpy())\n","\n","        predictions_array = np.concatenate(predictions)\n","\n","        # 예측값을 원래 스케일로 복원\n","        price_column_index = test_data.columns.get_loc(test_dataset.price_column)\n","        predictions_reshaped = predictions_array.reshape(-1, 1)\n","\n","        # 가격 열에 대해서만 inverse_transform 적용\n","        price_scaler = MinMaxScaler()\n","        price_scaler.min_ = 품목별_scalers[품목명].min_[price_column_index]\n","        price_scaler.scale_ = 품목별_scalers[품목명].scale_[price_column_index]\n","        predictions_original_scale = price_scaler.inverse_transform(predictions_reshaped)\n","        #print(predictions_original_scale)\n","\n","        if np.isnan(predictions_original_scale).any():\n","            pbar_inner.set_postfix({\"상태\": \"NaN\"})\n","        else:\n","            pbar_inner.set_postfix({\"상태\": \"정상\"})\n","            품목_predictions.extend(predictions_original_scale.flatten())\n","\n","\n","    품목별_predictions[품목명] = 품목_predictions\n","    pbar_outer.update(1)"]},{"cell_type":"code","execution_count":65,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":401},"executionInfo":{"elapsed":418,"status":"error","timestamp":1733299627575,"user":{"displayName":"김민서","userId":"06054877758271253489"},"user_tz":-540},"id":"f3uuyi349gmC","outputId":"b3efa208-2edf-4d87-dae8-395f3a91865d"},"outputs":[{"ename":"ValueError","evalue":"Length of values (25) does not match length of index (75)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-65-52f98c9d69d6\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 3\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m품목명\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m품목별_predictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 4\u001b[0;31m     \u001b[0msample_submission\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m품목명\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 결과 저장\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4309\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4310\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 4311\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4313\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u003e\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4522\u001b[0m         \u001b[0mensure\u001b[0m \u001b[0mhomogeneity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4523\u001b[0m         \"\"\"\n\u001b[0;32m-\u003e 4524\u001b[0;31m         \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4526\u001b[0m         if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   5264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 5266\u001b[0;31m             \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire_length_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5267\u001b[0m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5268\u001b[0m         if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/common.py\u001b[0m in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    571\u001b[0m     \"\"\"\n\u001b[1;32m    572\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 573\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    574\u001b[0m             \u001b[0;34m\"Length of values \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0;34mf\"({len(data)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Length of values (25) does not match length of index (75)"]}],"source":["sample_submission = pd.read_csv('./sample_submission.csv')\n","\n","for 품목명, predictions in 품목별_predictions.items():\n","    sample_submission[품목명] = predictions\n","\n","# 결과 저장\n","sample_submission.to_csv('./baseline_submission11.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U3BfhM-3jU25"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPfM9piZQ/xmOkOft2m+z1G","gpuType":"T4","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0ea4cada858f457791845e7e5183f52d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"125dd3e635714adaa5268fd838908a05":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fda966b1a1a64cb298a1719137cc5724","IPY_MODEL_9dbba9ca91064fc5a85d24554c6a7f78","IPY_MODEL_96446a910815442b8279b5630f708e14"],"layout":"IPY_MODEL_31c2f08cd81e44f798fa8cc10675cf64"}},"19b7c057c1964684a831d3fa0df6b823":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_b7b918548f7f40f197cd4c01c5fcedb1","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bb5da8a1b31e44648cd7f9ff0de2cbdc","value":23}},"210362621fbc4b759f102a1cf14d85b0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"2cb80b684e694a9cb054c8aeb723d3f2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_33820d430dd847838a2bea0e4d439dd1","IPY_MODEL_50c84e242b0146aa9f6f25ad34543f6f","IPY_MODEL_53372bfa58ee4d00a7b369e2e23efdba"],"layout":"IPY_MODEL_ecaf4b03264846c8974f9d1eb22f7167"}},"30a8fad335904b07abd4ecc710935f6f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d68e8e7335fb4b9589ccfe84e4de3714","IPY_MODEL_dc254441e4e34d1ca3e634e908d01dee","IPY_MODEL_77583f4c63d14800b50a8c5440e9d2c9"],"layout":"IPY_MODEL_210362621fbc4b759f102a1cf14d85b0"}},"31c2f08cd81e44f798fa8cc10675cf64":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"321cc9bddaa74ae7aff1c975fa18460a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"33820d430dd847838a2bea0e4d439dd1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_deb5eb47482a4c4ea76822f39becbf2b","placeholder":"​","style":"IPY_MODEL_321cc9bddaa74ae7aff1c975fa18460a","value":"테스트 파일 추론 중: 100%"}},"39a2cc9f2b06434280b134135e07439c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3bb47c43531c4d81b956114612fc3569":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3df41bfbc6f04ad6ad514f5c0edc08f4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"449e48e8953c411fa0197469bdb18191":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"45978aeff2e945c69f25fc05d311402d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f53ede88e9964279918f8fdd8bda2583","IPY_MODEL_19b7c057c1964684a831d3fa0df6b823","IPY_MODEL_711ad4413c43413e893a7a4c07cbe39b"],"layout":"IPY_MODEL_3bb47c43531c4d81b956114612fc3569"}},"4908247eed4a4aa3a2f37468313b2fa4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"49b37be89b3c420cb3a3c8925702e18e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5bc7651037db44c4ac87dad54ad48d72","IPY_MODEL_6b80e4bd52d046708152a6156d4998d0","IPY_MODEL_62964b7000c54368a4074f46e77a2e76"],"layout":"IPY_MODEL_87e490008a804d62a1aa136b54fc43fb"}},"50c84e242b0146aa9f6f25ad34543f6f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_5dc3c2bfe03246739c33022d983e9f94","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_65c671d457484d8e84c2fe227268d626","value":25}},"53372bfa58ee4d00a7b369e2e23efdba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4908247eed4a4aa3a2f37468313b2fa4","placeholder":"​","style":"IPY_MODEL_aab3ef75b3c440be9abe5911f6dce024","value":" 25/25 [00:05\u0026lt;00:00,  4.03it/s, 상태=정상]"}},"5bc7651037db44c4ac87dad54ad48d72":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_449e48e8953c411fa0197469bdb18191","placeholder":"​","style":"IPY_MODEL_39a2cc9f2b06434280b134135e07439c","value":"테스트 파일 추론 중: 100%"}},"5db43e45ce914ddfbdf234d3288c2c3a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5dc3c2bfe03246739c33022d983e9f94":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"62964b7000c54368a4074f46e77a2e76":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ada80da9b0834850984780889b2945b4","placeholder":"​","style":"IPY_MODEL_5db43e45ce914ddfbdf234d3288c2c3a","value":" 25/25 [00:15\u0026lt;00:00,  1.58it/s, 상태=정상]"}},"65c671d457484d8e84c2fe227268d626":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"689e5ee2aede41268c52a992f112ed12":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b49b834b323427b944417c34268e410":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_689e5ee2aede41268c52a992f112ed12","placeholder":"​","style":"IPY_MODEL_cb66dccf6c474ae58768b6b07ea2bc36","value":"테스트 파일 추론 중: 100%"}},"6b80e4bd52d046708152a6156d4998d0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_af3354954dc34070aed23c8e92fdc2c5","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6bef540bf2044961abf76bcb2bba3af1","value":25}},"6b826f67ceee4d618237d511f02e0b33":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6bef540bf2044961abf76bcb2bba3af1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"711ad4413c43413e893a7a4c07cbe39b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ad80d5a5bfa6435baa6aa1e9b7e22a70","placeholder":"​","style":"IPY_MODEL_ff19626d2b294931b22d01b45e8c5adf","value":" 22/25 [00:14\u0026lt;00:01,  1.59it/s, 상태=정상]"}},"77583f4c63d14800b50a8c5440e9d2c9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0ea4cada858f457791845e7e5183f52d","placeholder":"​","style":"IPY_MODEL_9b006fce652b41bf96310586afca3dc8","value":" 25/25 [00:03\u0026lt;00:00,  6.32it/s, 상태=정상]"}},"78cf452376ec4f6d8dddd8387b16f24b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7926fa3857b942c58b188ac45ee9dc6a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e4f14d9971cd45fea9b6c351c442cfac","placeholder":"​","style":"IPY_MODEL_fe1d5567d6514cf5a2cd362da3fbc7cc","value":" 8/10 [01:02\u0026lt;00:14,  7.25s/it]"}},"7c3cb7d55bee4ce8a057789a0d43aa65":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8501452dbedb42ea8146fcd70b39319a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"87e490008a804d62a1aa136b54fc43fb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"90067b3d2a5b451f94ad0eacbe14f665":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"96446a910815442b8279b5630f708e14":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f72ce947782b4cc5909dcc903ee77704","placeholder":"​","style":"IPY_MODEL_a4337c6693cd4698b3fbe9cda56b0c9c","value":" 25/25 [00:03\u0026lt;00:00,  6.70it/s, 상태=정상]"}},"9b006fce652b41bf96310586afca3dc8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9b4087141d6441be8bd69418d95cdcdd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_ce02030cddb54dba9934c56092e2b2ff","max":10,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ac02b8f10d50448b9460114f621ea0a7","value":8}},"9dbba9ca91064fc5a85d24554c6a7f78":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_c06145367b1d47fa8e5938a13fe33107","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bedf09abc577436db0f250080ffd49f2","value":25}},"a217bb10e5c245459b8e4b5945401d7a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a4337c6693cd4698b3fbe9cda56b0c9c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aab3ef75b3c440be9abe5911f6dce024":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ac02b8f10d50448b9460114f621ea0a7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ad80d5a5bfa6435baa6aa1e9b7e22a70":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ada80da9b0834850984780889b2945b4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af3354954dc34070aed23c8e92fdc2c5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0680d3980ae449e84476be606de6633":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2ae92c0564c4b1b83437044f5389635":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_a217bb10e5c245459b8e4b5945401d7a","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cfd583b96a0f4c0dabd2a0b1a0d56651","value":25}},"b7b918548f7f40f197cd4c01c5fcedb1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bb5da8a1b31e44648cd7f9ff0de2cbdc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bedf09abc577436db0f250080ffd49f2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c06145367b1d47fa8e5938a13fe33107":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c4db7f47153e4047b1b44dea16203db7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c55febca67954d52b8babf6ebf39cc0c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7c3cb7d55bee4ce8a057789a0d43aa65","placeholder":"​","style":"IPY_MODEL_90067b3d2a5b451f94ad0eacbe14f665","value":" 25/25 [00:04\u0026lt;00:00,  5.40it/s, 상태=정상]"}},"c6fb85e79a88432cae633f6343e75a19":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c8fb417e48a5418f8f83349caa14528a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cb66dccf6c474ae58768b6b07ea2bc36":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ce02030cddb54dba9934c56092e2b2ff":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cfd583b96a0f4c0dabd2a0b1a0d56651":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d22a378c1d2b41c4b3a185b79111e913":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5ad6fdced5b49129c4f0ce94251b9ff":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d976a305ec0c4f25b2f733949541f07e","IPY_MODEL_9b4087141d6441be8bd69418d95cdcdd","IPY_MODEL_7926fa3857b942c58b188ac45ee9dc6a"],"layout":"IPY_MODEL_f1b387c4f65b4d43b66402ba82ec1b21"}},"d68e8e7335fb4b9589ccfe84e4de3714":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3df41bfbc6f04ad6ad514f5c0edc08f4","placeholder":"​","style":"IPY_MODEL_6b826f67ceee4d618237d511f02e0b33","value":"테스트 파일 추론 중: 100%"}},"d976a305ec0c4f25b2f733949541f07e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d22a378c1d2b41c4b3a185b79111e913","placeholder":"​","style":"IPY_MODEL_c8fb417e48a5418f8f83349caa14528a","value":"품목별 전처리 및 모델 학습 -\u0026gt; 무:  80%"}},"dc254441e4e34d1ca3e634e908d01dee":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_b0680d3980ae449e84476be606de6633","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ddafca916a1a42aa80717b0962d6b6f0","value":25}},"ddafca916a1a42aa80717b0962d6b6f0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"deb5eb47482a4c4ea76822f39becbf2b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e4f14d9971cd45fea9b6c351c442cfac":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e5e5169e1ccb4913926ecb0264343a76":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"ead76133abc646749ae664a662913518":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6b49b834b323427b944417c34268e410","IPY_MODEL_b2ae92c0564c4b1b83437044f5389635","IPY_MODEL_c55febca67954d52b8babf6ebf39cc0c"],"layout":"IPY_MODEL_e5e5169e1ccb4913926ecb0264343a76"}},"ecaf4b03264846c8974f9d1eb22f7167":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"f1b387c4f65b4d43b66402ba82ec1b21":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f53ede88e9964279918f8fdd8bda2583":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8501452dbedb42ea8146fcd70b39319a","placeholder":"​","style":"IPY_MODEL_c4db7f47153e4047b1b44dea16203db7","value":"테스트 파일 추론 중:  88%"}},"f72ce947782b4cc5909dcc903ee77704":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fda966b1a1a64cb298a1719137cc5724":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c6fb85e79a88432cae633f6343e75a19","placeholder":"​","style":"IPY_MODEL_78cf452376ec4f6d8dddd8387b16f24b","value":"테스트 파일 추론 중: 100%"}},"fe1d5567d6514cf5a2cd362da3fbc7cc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ff19626d2b294931b22d01b45e8c5adf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}